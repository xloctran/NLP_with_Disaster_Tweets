{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter has become an important communication channel in times of emergency.\n",
    "The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n",
    "But, it’s not always clear whether a person’s words are actually announcing a disaster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has 7613 rows and 5 columns\n"
     ]
    }
   ],
   "source": [
    "# What is the shape of the data\n",
    "print('Input has {} rows and {} columns'.format(len(data),len(data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out 7613 rows, 3271 are disaster and 4342 are not disaster\n"
     ]
    }
   ],
   "source": [
    "# How many disaster and non disaster are there?\n",
    "print('Out {} rows, {} are disaster and {} are not disaster'.format(len(data),len(data[data['target']==1]),len(data[data['target']==0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clearning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "ps = nltk.PorterStemmer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    token = re.split('\\W+',text)\n",
    "    text = [ps.stem(word) for word in token if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \n",
       "0       1  [deed, reason, earthquak, may, allah, forgiv, us]  \n",
       "1       1       [forest, fire, near, la, rong, sask, canada]  \n",
       "2       1  [resid, ask, shelter, place, notifi, offic, ev...  \n",
       "3       1  [13000, peopl, receiv, wildfir, evacu, order, ...  \n",
       "4       1  [got, sent, photo, rubi, alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_text']= data['text'].apply(lambda x: clean_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Our Deeds are the Reason of this #earthquake M...\n",
      "1                  Forest fire near La Ronge Sask. Canada\n",
      "2       All residents asked to 'shelter in place' are ...\n",
      "3       13,000 people receive #wildfires evacuation or...\n",
      "4       Just got sent this photo from Ruby #Alaska as ...\n",
      "                              ...                        \n",
      "7608    Two giant cranes holding a bridge collapse int...\n",
      "7609    @aria_ahrary @TheTawniest The out of control w...\n",
      "7610    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
      "7611    Police investigating after an e-bike collided ...\n",
      "7612    The Latest: More Homes Razed by Northern Calif...\n",
      "Name: text, Length: 7613, dtype: object\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "X= data['text']\n",
    "y= data.target.to_numpy()\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7502        @raineishida lol...Im just a nervous wreck :P\n",
      "1368    @POTUS Would you please explain what you are g...\n",
      "2366    @shantaeskyy GM! I pray any attack of the enem...\n",
      "7102    After a violent afternoon storm more severe we...\n",
      "3130    Not being able to touch anything or anyone in ...\n",
      "                              ...                        \n",
      "2112    I had no issues uploading DEATH TO SMOOCHY or ...\n",
      "5652    UD: Rescue (Structural Collapse) - Scott Road ...\n",
      "2619    There's a #fly loose in my workspace with two ...\n",
      "517     #WeLoveLA #NHLDucks Avalanche Defense: How The...\n",
      "6518    It's going on three years that we have been se...\n",
      "Name: text, Length: 5329, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=46)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5329, 14935)\n",
      "['', '0', '0011', '001116', '0025', '010217', '0104', '010401', '012624', '02']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "X_counts_train = count_vect.fit_transform(X_train)\n",
    "X_counts_test = count_vect.transform(X_test)\n",
    "print(X_counts_train.shape)\n",
    "print(count_vect.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 198)\n",
      "['', '2', '2011', '213924', '25', '300w', '30stm', '4', '40', '4x4', '53inch', 'abl', 'accid', 'acdelco', 'affect', 'afternoon', 'agenc', 'air', 'airplan', 'amp', 'annihil', 'anyon', 'anyth', 'around', 'attack', 'audienc', 'august', 'b1g', 'bar', 'bickleton', 'binladen', 'bless', 'block', 'bmurph1019', 'build', 'burn', 'bush', 'camp', 'cant', 'chang', 'chicago', 'chonc', 'co2', 'combo', 'comment', 'corner', 'cree', 'cross', 'crowd', 'croydon', 'curv', 'day', 'dec', 'deploy', 'derail', 'destini', 'detail', 'deton', 'disea', 'due', 'electrocut', 'emerg', 'enemi', 'equip', 'evacu', 'even', 'excus', 'explain', 'famili', 'fatal', 'feel', 'final', 'fire', 'flood', 'fog', 'forest', 'full', 'get', 'gm', 'go', 'goblu', 'good', 'got', 'great', 'gt', 'h20', 'hailyoutsey', 'head', 'high', 'home', 'hors', 'httpstcocvkqigr1az', 'httptco6peeip4y7w', 'httptco9vd6x4wdoy', 'httptcodywwnbbyvj', 'httptcogxyivswki7', 'httptcohpzhe0cjvf', 'httptcomnsy1qr7bq', 'httptcomtmoia0oo0', 'httptconn4ztcmsru', 'httptcooaskgki6qj', 'httptcosaohvjktqc', 'httptcosb5r7shccj', 'httptcowrb7xd8w5i', 'hurrican', 'ignit', 'im', 'kca', 'kill', 'knock', 'lamp', 'led', 'legionnair', 'lembra', 'life', 'light', 'liveonk2', 'lolim', 'lord', 'mbataweel', 'member', 'michigan', 'much', 'nation', 'nervou', 'next', 'offroad', 'origin', 'outbreak', 'p', 'park', 'penney', 'permit', 'plan', 'pleas', 'potu', 'pray', 'properli', 'quem', 'raineishida', 'red', 'reev', 'reopen', 'riot', 'rip', 'river', 'roosevelt', 'salt', 'school', 'scream', 'seek', 'seismic', 'sensor', 'seri', 'servic', 'sever', 'shantaeskyy', 'shelter', 'spell', 'spot', 'spout', 'stop', 'stori', 'storm', 'street', 'sue', 'surviv', 'techniqu', 'termn8r13', 'thank', 'thegam', 'thx', 'tonight', 'tonto', 'touch', 'transport', 'traumatis', 'troop', 'typo', 'upheav', 'ur', 'us', 'via', 'violent', 'volcano', 'votejkt48id', 'wa', 'weather', 'what', 'wheavenli', 'wild', 'wildfir', 'without', 'work', 'would', 'wreck', 'wrestleon', 'û']\n"
     ]
    }
   ],
   "source": [
    "# Apply CountVectorizer to small sample\n",
    "X_train_sample= X_train[0:20]\n",
    "count_vect_sample = CountVectorizer(analyzer=clean_text)\n",
    "X_counts_sample = count_vect_sample.fit_transform(X_train_sample)\n",
    "print(X_counts_sample.shape)\n",
    "print(count_vect_sample.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>2011</th>\n",
       "      <th>213924</th>\n",
       "      <th>25</th>\n",
       "      <th>300w</th>\n",
       "      <th>30stm</th>\n",
       "      <th>4</th>\n",
       "      <th>40</th>\n",
       "      <th>4x4</th>\n",
       "      <th>...</th>\n",
       "      <th>what</th>\n",
       "      <th>wheavenli</th>\n",
       "      <th>wild</th>\n",
       "      <th>wildfir</th>\n",
       "      <th>without</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>wreck</th>\n",
       "      <th>wrestleon</th>\n",
       "      <th>û</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       2  2011  213924  25  300w  30stm  4  40  4x4  ...  what  wheavenli  \\\n",
       "0   0  0     0       0   0     0      0  0   0    0  ...     0          0   \n",
       "1   0  0     0       0   0     0      0  0   0    0  ...     0          0   \n",
       "2   0  1     0       0   0     0      0  0   0    0  ...     0          1   \n",
       "3   0  0     0       0   0     0      0  0   0    0  ...     0          0   \n",
       "4   1  0     0       0   0     0      0  0   0    0  ...     0          0   \n",
       "5   0  0     0       0   0     0      0  0   0    0  ...     0          0   \n",
       "6   0  0     1       0   0     0      0  0   0    0  ...     0          0   \n",
       "7   0  0     0       0   0     0      0  0   0    0  ...     0          0   \n",
       "8   0  0     0       0   0     0      0  1   0    0  ...     0          0   \n",
       "9   0  0     0       0   0     0      0  0   0    0  ...     0          0   \n",
       "10  0  0     0       0   0     0      0  0   0    0  ...     0          0   \n",
       "11  0  0     0       0   0     0      0  0   0    0  ...     0          0   \n",
       "12  0  0     0       0   0     0      0  0   1    0  ...     0          0   \n",
       "13  0  0     0       0   0     1      0  0   0    1  ...     0          0   \n",
       "14  0  0     0       0   1     0      0  0   0    0  ...     0          0   \n",
       "15  0  0     0       0   0     0      0  0   0    0  ...     1          0   \n",
       "16  0  0     0       1   0     0      0  0   0    0  ...     0          0   \n",
       "17  0  0     0       0   0     0      0  0   0    0  ...     0          0   \n",
       "18  0  0     0       0   0     0      0  0   0    0  ...     0          0   \n",
       "19  0  0     0       0   0     0      1  0   0    0  ...     0          0   \n",
       "\n",
       "    wild  wildfir  without  work  would  wreck  wrestleon  û  \n",
       "0      0        0        0     0      0      1          0  0  \n",
       "1      0        0        0     0      1      0          0  0  \n",
       "2      0        0        0     0      0      0          0  0  \n",
       "3      0        0        0     0      0      0          0  0  \n",
       "4      0        0        1     0      0      0          0  0  \n",
       "5      0        0        0     0      0      0          0  0  \n",
       "6      0        0        0     0      0      0          0  0  \n",
       "7      1        0        0     0      0      0          0  0  \n",
       "8      0        0        0     0      0      0          0  0  \n",
       "9      0        0        0     0      0      0          0  0  \n",
       "10     0        0        0     0      0      0          0  0  \n",
       "11     0        0        0     0      0      0          0  0  \n",
       "12     0        0        0     0      0      0          0  0  \n",
       "13     0        0        0     1      0      0          0  1  \n",
       "14     0        1        0     0      0      0          0  0  \n",
       "15     0        0        0     0      0      0          0  0  \n",
       "16     0        0        0     0      0      0          0  0  \n",
       "17     0        0        0     0      0      0          1  0  \n",
       "18     0        0        0     0      0      0          0  0  \n",
       "19     0        0        0     0      0      0          0  0  \n",
       "\n",
       "[20 rows x 198 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_counts_df = pd.DataFrame(X_counts_sample.toarray())\n",
    "X_counts_df.columns = count_vect_sample.get_feature_names()\n",
    "X_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9283167573653593\n",
      "0.7985989492119089\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_counts_train, y_train)\n",
    "print(clf.score(X_counts_train,y_train))\n",
    "print(clf.score(X_counts_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5329, 14935)\n",
      "['', '0', '0011', '001116', '0025', '010217', '0104', '010401', '012624', '02']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf_train = tfidf_vect.fit_transform(X_train)\n",
    "X_tfidf_test  = tfidf_vect.transform(X_test)\n",
    "print(X_tfidf_train.shape)\n",
    "print(tfidf_vect.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.903921936573466\n",
      "0.8021015761821366\n"
     ]
    }
   ],
   "source": [
    "clf_tfidf = MultinomialNB()\n",
    "clf_tfidf.fit(X_tfidf_train, y_train)\n",
    "print(clf_tfidf.score(X_tfidf_train,y_train))\n",
    "print(clf_tfidf.score(X_tfidf_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N- Grams Vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "ps = nltk.PorterStemmer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def clean_text_NGram(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    token = re.split('\\W+',text)\n",
    "    text = \" \".join([ps.stem(word) for word in token if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_NGram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order califor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \\\n",
       "0       1  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1       1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2       1  [resid, ask, shelter, place, notifi, offic, ev...   \n",
       "3       1  [13000, peopl, receiv, wildfir, evacu, order, ...   \n",
       "4       1  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "                                  cleaned_text_NGram  \n",
       "0          deed reason earthquak may allah forgiv us  \n",
       "1               forest fire near la rong sask canada  \n",
       "2  resid ask shelter place notifi offic evacu she...  \n",
       "3  13000 peopl receiv wildfir evacu order califor...  \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_text_NGram']= data['text'].apply(lambda x: clean_text_NGram(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               deed reason earthquak may allah forgiv us\n",
      "1                    forest fire near la rong sask canada\n",
      "2       resid ask shelter place notifi offic evacu she...\n",
      "3       13000 peopl receiv wildfir evacu order califor...\n",
      "4       got sent photo rubi alaska smoke wildfir pour ...\n",
      "                              ...                        \n",
      "7608    two giant crane hold bridg collaps nearbi home...\n",
      "7609    ariaahrari thetawniest control wild fire calif...\n",
      "7610    m194 0104 utc5km volcano hawaii httptcozdtoyd8ebj\n",
      "7611    polic investig ebik collid car littl portug eb...\n",
      "7612    latest home raze northern california wildfir a...\n",
      "Name: cleaned_text_NGram, Length: 7613, dtype: object\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "X= data['cleaned_text_NGram']\n",
    "y= data.target.to_numpy()\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7502                     raineishida lolim nervou wreck p\n",
      "1368    potu would pleas explain go volcano amp bush f...\n",
      "2366    shantaeskyy gm pray attack enemi 2 derail ur d...\n",
      "7102    violent afternoon storm sever weather head chi...\n",
      "3130     abl touch anyth anyon penney without electrocut \n",
      "                              ...                        \n",
      "2112    issu upload death smoochi awaken clip youtub r...\n",
      "5652    ud rescu structur collaps scott road ypre road...\n",
      "2619    there fli loos workspac two bore cat forse ter...\n",
      "517     welovela nhlduck avalanch defens match vs st l...\n",
      "6518    go three year separ sometim let man know leav ...\n",
      "Name: cleaned_text_NGram, Length: 5329, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=46)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5329, 52809)\n",
      "['0011', '0011 utc', '001116', '001116 utc20150805', '0025', '0025 updat', '010217', '010217 okinawa', '0104', '0104 utc']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ngram_vect = CountVectorizer(ngram_range=(1,2))\n",
    "X_ngram_train = ngram_vect.fit_transform(X_train)\n",
    "X_ngram_test = ngram_vect.transform(X_test)\n",
    "print(X_ngram_train.shape)\n",
    "print(ngram_vect.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9787952711578157\n",
      "0.8003502626970228\n"
     ]
    }
   ],
   "source": [
    "clf_ngram = MultinomialNB()\n",
    "clf_ngram.fit(X_ngram_train, y_train)\n",
    "print(clf_ngram.score(X_ngram_train,y_train))\n",
    "print(clf_ngram.score(X_ngram_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8588853443422781\n",
      "0.7994746059544658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_log = LogisticRegression(random_state=0, C=0.5).fit(X_tfidf_train, y_train)\n",
    "print(clf_log.score(X_tfidf_train,y_train))\n",
    "print(clf_log.score(X_tfidf_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
